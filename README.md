# Information-Retrieval
정보검색


팀을 만들어 두었습니다

  https://github.com/orgs/BlockChain-Information/teams/information-retrieval  여기로 접속
  
  
# 주제 : Image Captioning


----
## Description
* connects computer vision and Natual language Processing
* 이미지를 읽어줌  
----
## Dataset
1. Flickr30k Dataset has been used for the training of model.
   [Flickr30K](https://www.kaggle.com/hsankesara/flickr-image-dataset)
2. Glove6B dataset [Link](https://drive.google.com/open?id=1GI5sWeCxgJEgToeVmakL69oDlXowXGU4)
----
## Reference Paper
* Paper:[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf) [요약](https://mrsyee.github.io/nlp/2018/11/24/Show_and_tell/)
* Paper:[Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/pdf/1412.2306.pdf)
----
## Requirements
* Python
* Numpy

## Project Structure


## Model flow Chart

## Run Model in steps
---
*Image Feature Extraction*
--------------------------

*Caption Preprocessing step*
--------------------------

*Generating Vocabulary of Caption words*
--------------------------

*Training of the Model*
--------------------------

*Test model*
--------------------------

## Testing The Model
---
model can be tested using below command after clonning the repository


*Test Results*
--------------

  
